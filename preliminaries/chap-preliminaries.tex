\chapter{Set up and Preliminaries}\label{chap:preliminaries}

\epigraph{``Skip all that!'' cried the Bellman in haste.\\
If it once becomes dark, there's no chance of a Snark--\\
We have hardly a minute to waste!''}{Lewis Carroll, \textit{The Hunting of the Snark}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation.} 

All throughout this thesis, we denote by $[n]$ the set $\{1,\dots,n\}$, and by $\llbracket n\rrbracket$ the set $\{0,\dots,n-1\}$. We will write $\log$  (resp. $\ln$) for the binary logarithm (resp. the natural logarithm). Besides the standard asymptotic conventions, we use the notations $\tildeO{f},\tildeOmega{f}$ to hide polylogarithmic dependencies on the argument, and will sometimes write $O_\eps(f)$ to signify that the hidden constant depends on the parameter $\eps$ (while $f$ does not).

We now formally introduce the main actor of this dissertation. A \emph{probability distribution} over a (countable) domain\footnote{For the sake of this thesis, all distributions will be supported on a finite or at least discrete domain; thus, we do not consider the fully general definitions from measure theory.} $\domain$ is a non-negative function $\D\colon\domain\to[0,1]$ such that $\sum_{x\in\domain} \D(x) = 1$. We denote by $\distribs{\domain}$ the (convex) polytope of all such distributions, and by $\uniformOn{\domain}$ the uniform distribution on \domain (when well-defined); when clear from context, we may sometimes omit the domain and simply write $\uniform$. Given a distribution $\D$ over $\domain$ and a set $S\subseteq\domain$, we write $\D(S)$ for the total probability weight $\sum_{x\in S} \D(x)$ assigned to $S$ by $\D$. Moreover, for $S \subseteq \domain$ such that $\D(S)>0$, we denote by $\D_S$ the conditional distribution of $\D$ restricted to $S$, that is $\D_S(x) = \frac{\D(x)}{\D(S)}$ for $x \in S$ and $\D_S(x)=0$ otherwise. We also let $\supp\D\eqdef\setOfSuchThat{ x\in\Omega }{ \D(x) > 0 }$ be the \emph{(effective) support} of the distribution, i.e. the subset of the domain to which $\D$ assigns non-zero probability weight . Finally, for a probability distribution $\D\in\distribs{\domain}$ and integer $m$, we write $\D^{\otimes m}\in\distribs{\domain^m}$ for the $m$-fold product distribution obtained by drawing $m$ independent samples $s_1,\dots,s_m\sim \D$ and outputting $(s_1,\dots,s_m)$.

When the domain is a subset of the natural numbers $\N$, we shall often abuse notation and identify a distribution $\p\in\distribs{\domain[\Omega]}$ with the sequence $(\p_i)_{i\in\domain}\in\lp[1]$ corresponding to its probability mass function (pmf).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Property testing, distributions, and metrics.}\label{sec:preliminaries:basics}

As is usual in property testing of distributions, throughout this dissertation the distance between two distributions $\D_1, \D_2\in \distribs{\domain}$ will be the \emph{total variation distance}:
\begin{equation}\label{def:distance:tv}
\totalvardist{\D_1}{\D_2 } \eqdef \max_{S \subseteq \domain} (\D_1(S)-\D_2(S)) = \frac{1}{2} \sum_{x \in \domain}\abs{\D_1(i)-\D_2(i)} = \frac{1}{2} \normone{\D_1 - \D_2}
\end{equation}
which takes value in $[0,1]$. (Due to the equivalence between total variation and $\lp[1]$ distances, we will sometimes phrase our results in terms of the latter, and ask the reader for their forgiveness.) In some cases, it is useful to consider -- either as a proxy towards total variation, or for the sake of the analysis -- different metrics, such as $\lp[2]$, Kolmogorov, or Hellinger distances. More on these can be found in~\cref{chap:preliminaries:previous:tools}.

A property $\property$ of distributions over $\domain$ is then simply a subset of $\distribs{\domain}$, consisting of all distributions that have the property. The distance from $\D$ to a property $\property$, denoted $\totalvardist{\D}{\property}$, is then defined as $\inf_{\D^\prime \in \property} \totalvardist{\D}{\D^\prime}$. Given a distribution $\D$ and a property $\property$, we say that $\D$ is \emph{\eps-close} to $\property$ if $\totalvardist{\D}{\property}\leq \eps$; otherwise, $\D$ is \emph{\eps-far} from $\property$. We shall oftentimes refer to some properties as ``classes'' of distribution, trading \property for the symbol \class; specifically, this will be the case for \emph{structured} properties of distributions, in keeping with the existing literature.

We recall the standard definition of testing algorithms for properties of distributions over $\domain$, where $n$ is the relevant parameter for $\domain$ (i.e., in most cases, its size $\abs{\domain}$). To be consistent with the rest of this dissertation, we chose to phrase it in the most general setting possible, with regard to how the unknown distribution is ``queried'': and will specify this aspect further in the relevant chapters (sampling access, conditional access, etc.).
\begin{definition}\label{def:testing}
Let $\property$ be a property of distributions over $\domain$.  Let $\ORACLE_\D$ be an oracle providing some type of access to $\D$. A \emph{$q$-query testing algorithm for $\property$} (for this type of oracle) is a randomized algorithm $\Tester$ which takes as input $n\in\N$, $\eps\in(0,1)$, as well as access to $\ORACLE_\D$.  After making at most $q(\eps,n)$ calls to the oracle, \Tester either outputs \accept or \reject, such that the following holds:
\begin{itemize}
\item if $\D \in \property$, then with probability at least $2/3$, \Tester outputs \accept;
\item if $\totalvardist{\D}{\property} > \eps$, then with probability at least $2/3$, \Tester outputs \reject;
\end{itemize}
where the probability is taken over the algorithm's randomness and (if any) the randomness from the oracle's answers.
\end{definition}
The most common type of oracle is the ``sampling oracle,'' which provides access to independent samples drawn from $\D$. Besides this standard definition of testing algorithms, we will also be interested in a generalization, that of \emph{tolerant} testers -- roughly, algorithms robust to a relaxation of the first item above:
\begin{definition}\label{def:tol:testing}
  Let $\property$ and $\ORACLE_\D$ be as above. A \emph{$q$-query tolerant testing algorithm for $\property$} is a randomized algorithm $\Tester$ which takes as input $n\in\N$, $0 \leq \eps_1 < \eps_2 \leq 1$, as well as access to $\ORACLE_D$. After making at most $q(\eps_1,\eps_2,n)$ calls to the oracle, \Tester outputs either \accept or \reject, such that the following holds:
  \begin{itemize}
    \item if $\totalvardist{\D}{\property} \leq \eps_1$, then with probability at least $2/3$, \Tester outputs \accept;
    \item if $\totalvardist{\D}{\property} \geq \eps_2$, then with probability at least $2/3$, \Tester outputs \reject;
  \end{itemize}
where the probability is taken over the algorithm's randomness and (if any) the randomness from the oracle's answers.
\end{definition}
Note that these definitions in particular do not specify the behavior of the algorithms when $\totalvardist{\D}{\property} \in (0,\eps)$ (resp. $\totalvardist{\D}{\property} \in (\eps_1,\eps_2)$): in this case, any answer from the tester is considered valid. Furthermore, we stress that the two definitions above only deal with the query complexity, and not the running time. Almost every lower bound will however apply to computationally unbounded algorithms, while most upper bounds we will cover are achieved by testing algorithms whose running time is polynomial in the number of queries they make.
\footnotetext{Note that, as standard in property testing, the threshold $2/3$ is arbitrary: any $1-\delta$ confidence can be achieved at the cost of a multiplicative factor $\log(1/\delta)$ in the query complexity, by repeating the test and outputting the majority vote.}

%%%%%%%%%%%%%%%
\definecolor{yolk}{RGB}{249, 208, 16}
\begin{figure}[H]\centering
  \begin{tikzpicture}[decoration={random steps,segment length=1mm,amplitude=1.2pt}, scale=1.5]%[decoration=penciline]
    \draw[decorate] (2,2) ellipse (12mm and 14.4mm);
    \draw[decorate,fill=white] (2,2) ellipse (6mm and 7.2mm);
    \draw[decorate,pattern=north east lines, pattern color=gray] (2,2) ellipse (6mm and 7.2mm);
    \filldraw[decorate,color=yolk] (2,2) ellipse (4.5mm and 5.4mm);
    \draw[decorate] (2,2) ellipse (4.5mm and 5.4mm);
    \node at (2,2) {$\property$};
    
    \draw[decorate] (6,2) ellipse (12mm and 14.4mm);
    \draw[decorate,fill=white, draw=none] (6,2) ellipse (8mm and 9.6mm);
    \draw[decorate,pattern=north east lines, pattern color=gray] (6,2) ellipse (8mm and 9.6mm);
    \draw[decorate,fill=white] (6,2) ellipse (6mm and 7.2mm);
    \filldraw[decorate,fill=yolk] (6,2) ellipse (4.5mm and 5.4mm);
    \node at (6,2) {$\property$};
  \end{tikzpicture}
  \caption{\label{fig:testing:tolerant:testing}Testing vs. tolerant testing: the algorithm is off the hook whenever the unknown distribution belongs to the gray area. It looks like eggs, really.\index{egg}\index{egg|see {testing algorithm (tolerant)}}}
\end{figure}
%%%%%%%%%%%%%%%

A related notion is that of \emph{distance estimators}; that is, of algorithms which compute an approximation of the distance of the unknown distribution to a property.
\begin{definition}\label{def:distance:estimation}
  Let $\property$ and $\ORACLE_\D$ be as above. A \emph{$q$-query distance estimation algorithm for $\property$} is a randomized algorithm $\Algo$ which takes as input $n\in\N$, $\eps\in(0,1]$, as well as access to $\ORACLE_D$. After making at most $q(\eps,n)$ calls to the oracle, \Tester outputs a value $\gamma\in[0,1]$ such that,  with probability at least $2/3$, it holds that $\totalvardist{\D}{\property}\in[\gamma-\eps, \gamma+\eps]$.
\end{definition}

\begin{remark}[Tolerant testing and distance approximation]
Parnas, Ron, and Rubinfeld define and formalize in \cite{PRR:06} the notion of tolerant testing, and show that distance approximation and (fully)\footnote{I.e., tolerant testing algorithms as above that allow \emph{any} inputs $0 \leq \eps_1 < \eps_2 \leq 1$, without further restriction on the range of authorized values.} tolerant testing are equivalent, up to a logarithmic factor in $1/\eps$ in the sample complexity (Claims 1 and 2, Section 3.1).
\end{remark}

The last notion we shall require is that of distribution \emph{learning} (also referred to as \emph{density estimation}). The exact formalization of what \emph{learning a probability distribution} means has been considered in Kearns et al.~\cite{Kearns:94}. We note that in their language, the variant of learning this thesis considers is \emph{learning to generate}. We give the precise definition below:  
\begin{definition}
Let $\class\subseteq\distribs{\domain}$ be a class of probability distributions and $\D\in \class$ be an unknown distribution. Let also $\mathcal{H}$ be a hypothesis class of distributions. 
A \emph{$q$-sample learning algorithm for $\class$} is a randomized algorithm $\Learner$ which, given sample access to $\D$ and parameters $\eps,\delta\in(0,1)$,  outputs the description of a distribution $\hat{\D}\in \mathcal{H}$ such that with probability at least $1-\delta$ one has $\totalvardist{\D}{\hat{\D}}\leq \eps$. 

\noindent If in addition $\mathcal{H}\subseteq \class$, then we say \Learner is a \emph{proper} learning algorithm.   
\end{definition}
The above definition assumes that the probability distribution to be approximated belongs to a known class $\class$. However, in many cases and applications this assumption may not be exactly satisfied -- this is often referred to as ``model misspecification.'' In that case, one may still ask for a learning algorithm which would approximate $\D$ ``as well as the best distribution from $\class$.'' This generalization of the above notion of distribution learning is known as \emph{agnostic learning}:
\begin{definition}
Let $\class$  and $\mathcal{H}$ be as above. A \emph{(semi-)agnostic learning algorithm for \class} (using hypothesis class $\mathcal{H}$) is an algorithm \Algo which, given sample access to an arbitrary distribution $\D$ and parameters $\eps,\delta\in(0,1)$, outputs a hypothesis $\hat{\D}\in\mathcal{H}$ such that,  with probability at least $1-\delta$,
\[
	\totalvardist{\D}{\hat{\D}} \leq c\cdot\opt_{\class,\D} + \eps
\]
where $\opt_{\class,\D}\eqdef \inf_{\D'\in\class} \totalvardist{\D'}{\D}$ and $c\geq 1$ is an absolute constant (if $c=1$, the learner is said to be \emph{agnostic}).
\end{definition}

\paragraph{Generalization.} These definitions can easily be extended to cover situations in which there are two (or more) ``unknown'' distributions $\D_1,\D_2$ that are accessible respectively via $\ORACLE_{\D_1}$ and $\ORACLE_{\D_2}$ oracles. For instance, we shall consider algorithms for testing whether $\D_1 = \D_2$ versus  $\totalvardist{\D_1}{\D_2} > \eps$ in such a setting, the property now being formally a subset of $\distribs{\domain}\times\distribs{\domain}$.

\paragraph{On adaptivity and one-sidedness.} As usual in property testing, it is possible to specialize these definitions for some classes of algorithms. In particular, a tester which never errs when $\D\in\property$ (but is only allowed to be wrong with probability $1/3$ when $\D$ is far from \property) is said to be \emph{one-sided}; as defined above, testers are \emph{two-sided}. More important in this thesis is the notion of \emph{adaptive} testers: if an algorithm's queries do not depend on the previous answers made to the oracle(s), it is said to be \emph{non-adaptive}. However, if the $i$-th query can be a function of the $j$-th answer for $j< i$, then it is \emph{adaptive}. (Roughly speaking, a non-adaptive algorithm is one that can write down all the queries it is going to make ``in advance,'' only after tossing its own random coins).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classes of distributions}\label{ssec:class:definitions}

We give here the formal descriptions of the classes of distributions that shall appear in this dissertation, starting with that of monotone distributions.
\begin{definition}[monotone]\label{def:monotone}
A distribution $\D$ over $[n]$ is \emph{monotone} (non-increasing) if its probability mass function (pmf) satisfies $\D(1) \geq \D(2) \geq \dots \D(n)$.
\end{definition}
 A natural generalization of the class $\classmon$ of monotone distributions is the set of $t$-modal distributions, i.e. distributions whose pmf can go ``up and down'' or ``down and up'' up to $t$ times:\footnote{Note that this slightly deviates from the Statistics literature, where only the peaks are counted as modes (so that what is usually referred to as a bimodal distribution is, according to our definition, $3$-modal).}

\begin{definition}[$t$-modal]\label{def:tmodal}
  Fix any distribution $\D$ over $[n]$, and integer $t$. $\D$ is said to have $t$ \emph{modes} if there exists a sequence $i_0 < \dots < i_{t+1}$ such 
  that either $(-1)^j \D(i_j) < (-1)^j \D(i_{j+1})$ for all $0\leq j \leq t$, or $(-1)^j \D(i_j) > (-1)^j \D(i_{j+1})$ for all $0\leq j \leq t$. We call $\D$ \emph{$t$-modal} if it has at most $t$ modes, and write $\classtmo$ for the class of all $t$-modal distributions. The particular case of $t=1$ corresponds to the set $\classuni$ of \emph{unimodal} distributions.
\end{definition}

\begin{definition}[Log-concave]\label{def:logconcave}
  A distribution $\D$ over $[n]$ is said to be \emph{log-concave} if it satisfies the following conditions: \textsf{(i)} for any $1 \leq i < j < k \leq n$ such that $\D(i)\D(k) > 0$, $\D(j) > 0$; and \textsf{(ii)} for all $1 < k < n$, $\D(k)^2 \geq \D(k-1)\D(k+1)$. We write $\classlogconcave$ for the class of all log-concave distributions.
\end{definition}

\begin{definition}[Concave and Convex]\label{def:concave}
  A distribution $\D$ over $[n]$ is said to be \emph{concave} if it satisfies the following conditions: \textsf{(i)} for any $1 \leq i < j < k \leq n$ such that $\D(i)\D(k) > 0$, $\D(j) > 0$; and \textsf{(ii)} for all $1 < k < n$ such that $\D(k - 1)\D(k + 1)>0$, $2\D(k) \geq \D(k - 1)+\D(k + 1)$; it is \emph{convex} if the reverse inequality holds in \textsf{(ii)}. We write $\classcve$ (resp. $\classcvx$) for the class of all concave (resp. convex) distributions.
\end{definition}
It is easy to see that convex and concave distributions are unimodal; moreover, every concave distribution is also log-concave, i.e. $\classcve\subseteq\classlogconcave$. Note that in both \cref{def:logconcave} and \cref{def:concave}, condition \textsf{(i)} is equivalent to enforcing that the distribution be supported on an interval.

\begin{definition}[Monotone Hazard Rate]\label{def:mhr}
  A distribution $\D$ over $[n]$ is said to have \emph{monotone hazard rate} (MHR) if its \emph{hazard rate} $H(i)\eqdef \frac{\D(i)}{\sum_{j=i}^{n} \D(j)}$ is a non-decreasing function. We write $\classmhr$ for the class of all MHR distributions.
\end{definition}
It is known that every log-concave distribution is both unimodal and MHR (see e.g.~\cite[Proposition 10]{An:96}), and that monotone distributions are MHR. Two other classes of distributions have elicited significant interest in the context of density estimation, those of \emph{histograms} (piecewise constant) and \emph{piecewise polynomial densities}:
\begin{definition}[Piecewise Polynomials~\cite{CDSS:14}]\label{def:piecewise}
  A distribution $\D$ over $[n]$ is said to be a \emph{$t$-piecewise degree-$d$ distribution} if there is a partition of $[n]$ into $t$ disjoint intervals $I_1,\dots,I_t$ such that $\D(i) = p_j(i)$ for all $i \in I_j$, where each $p_1,\dots p_t$ is a univariate polynomial of degree at most $d$. We write $\classpoly$ for the class of all $t$-piecewise degree-$d$ distributions. (We note that {$t$-piecewise degree-$0$ distributions} are also commonly referred to as \emph{$t$-histograms}, and write $\classhist$ for $\classpoly[t,0]$.)
\end{definition}

Finally, we recall the definition of the two following classes, which both extend the family of Binomial distributions $\classbin[n]$: the first, by removing the need for each of the independent Bernoulli summands to share the same bias parameter.
\begin{definition}\label{def:pbd}
A random variable $X$ is said to follow a \emph{Poisson Binomial Distribution} (with parameter $n\in\N$) if it can be written as $X=\sum_{k=1}^n X_k$, where $X_1\dots,X_n$ are independent, non-necessarily identically distributed Bernoulli random variables. We denote by $\classpbd[n]$ the class of all such Poisson Binomial Distributions.
\end{definition}
\noindent It is not hard to show that Poisson Binomial Distributions are in particular log-concave. One can generalize even further, by allowing each random variable of the summation to be integer-valued:
\begin{definition}\label{def:siirv}
Fix any $k\geq 0$. We say a random variable $X$ is a \emph{$k$-Sum of Independent Integer Random Variables} with parameter $n\in\N$ ($(n,k)$-SIIRV) if it can be written as $X=\sum_{j=1}^n X_j$, where $X_1\dots,X_n$ are independent, non-necessarily identically distributed random variables taking value in $\modulo{k}$. We denote by $\classksiirv[n]{k}$ the class of all such $(n,k)$-SIIRVs.
\end{definition}
\noindent (The class of Poisson Binomial Distributions thus corresponds to the case $k=2$, that is $(n,2)$-SIIRVS.) A different type of generalization is that of Poisson Multinomial Distributions, where each summand is a random variable supported on the $k$ vectors of the standard basis of $\R^k$, instead of $[k]$:
\begin{definition}\label{def:pmd}
Fix any $k\geq 0$. We say a random variable $X$ is a \emph{$(n,k)$-Poisson Multinomial Distribution ($(n,k)$-PMD)} with parameter $n\in\N$ if it can be written as $X=\sum_{j=1}^n X_j$, where $X_1\dots,X_n$ are independent, non-necessarily identically distributed random variables taking value in $\{e_1,\dots,e_k\}$ (where $(e_i)_{i\in[k]}$ is the canonical basis of $\R^k$). We denote by $\classpmd[n]{k}$ the class of all such $(n,k)$-PMDs.
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Previous tools from the literature.}\label{chap:preliminaries:previous:tools}
As previously mentioned, in this thesis we will be concerned with the total variation distance between distributions. Of interest for the analysis of some of our algorithms, and assuming \domain is totally ordered (in our case, $\domain=[n]$), one can also define the \emph{Kolmogorov distance} between $\D_1$ and $\D_2$ as
\begin{equation}\label{eq:def:dk}
  \kolmogorov{\D_1}{\D_2} \eqdef \max_{x\in \domain}\abs{F_1(x)-F_2(x)}
\end{equation}
where $F_1$ and $F_2$ are the respective cumulative distribution functions (cdf) of $\D_1$ and $\D_2$. Thus, the Kolmogorov distance is the $\lp[\infty]$ distance between the cdf's; and $\kolmogorov{\D_1}{\D_2} \leq \totalvardist{\D_1}{\D_2} \in [0,1]$. \smallskip

We will also occasionally rely on the \emph{Hellinger distance}, a third metric on $\distribs{\domain}$ defined as
\[
\hellinger{\D_1}{\D_2} = \sqrt{ \frac{1}{2}\sum_{x\in \domain}\left(\sqrt{\D_1(x)}-\sqrt{\D_2(x)}\right)^2 } = \sqrt{ 1-\sum_{x\in \domain} \sqrt{\D_1(x)}\sqrt{\D_2(x)} } = \frac{1}{\sqrt{2}}\normtwo{\sqrt{\D_1} - \sqrt{\D_2}}
\]
which also takes values in $[0,1]$. One particularly useful feature of the Hellinger distance is its close relation to total variation:
\begin{fact}[{\cite[Corollary 2.39]{BarYossef:02}}]\label{theorem:tv:hellinger}
  For any probability distributions $\D_1$, $\D_2$ as above,
  \begin{equation}\label{eq:tv:hellinger}
    \totalvardist{\D_1}{\D_2}^2 \leq \hellinger{\D_1}{\D_2}^2 \leq \totalvardist{\D_1}{\D_2}
  \end{equation}
\end{fact}
\noindent For more on the Kolmogorov and Hellinger distances and their relation to total variation, we refer the reader to~\cite[Appendix C]{Canonne:15:Survey}.

On several occasions we will use the \emph{data processing inequality for variation distance}.  This intuitive yet fundamental result says that for any two distributions $\D$, $\D'$,
applying any (possibly randomized) function to both $\D$ and $\D'$ can never increase their statistical distance; see e.g. part~(iv) of~\cite[Lemma 2]{Rey:11} for a proof of this
lemma.
\begin{fact}[Data Processing Inequality for Total Variation Distance]\label{lemma:data:processing:inequality:total:variation}
Let $\D_1$, $\D_2$ be two distributions over a domain $\Omega$. Fix any randomized function\footnote{Which can be seen as a distribution over functions over $\Omega$.}{} $F$ on $\Omega$, and let $F(\D_1)$ be the distribution such that a draw from $F(\D_1)$ is obtained by drawing independently $x$ from $\D_1$ and $f$ from $F$ and then outputting $f(x)$ (likewise for $F(\D_2)$).
Then we have
\[
\totalvardist{ F(\D_1) }{  F(\D_2) }  \leq \totalvardist{ \D_1 }{ \D_2 }.
\]
\end{fact}

Finally, we recall below a fundamental fact from probability theory that will be useful to us, the \emph{Dvoretzky--Kiefer--Wolfowitz (DKW) inequality}. Informally, this result says that one can learn the cumulative distribution function of a distribution up to an additive error $\eps$ in $\lp[\infty]$ distance, by taking only $\bigO{1/\eps^2}$ samples from it.
\begin{theorem}[\cite{DKW:56,Massart:90}]\label{theo:dkw:ineq}
Let $\D$ be a distribution over $[n]$. Given $m$ independent samples $x_1,\dots ,x_m$ from $\D$, define the empirical distribution $\hat{\D}$ as follows:
\[
\hat{\D}(i) \eqdef \frac{\abs{ \setOfSuchThat{j\in[m]}{x_j=i} } }{m}, \quad i\in[n].
\]
Then, for all $\eps > 0$, $\probaOf{ \kolmogorov{\D}{\hat{\D}} > \eps } \leq 2e^{-2m\eps^2}$, where the probability is taken over the samples.
\end{theorem} 
\noindent In particular, setting $m=\bigTheta{\frac{\log(1/\delta)}{\eps^2}}$ we get that $\kolmogorov{\D}{\hat{\D}} \leq \eps$ with probability at least $1-\delta$.

\paragraph{Flattenings.}
\noindent For a distribution $\D$ and a partition of $[n]$ into intervals $\mathcal{I}=(I_1,\dots,I_\ell)$, we define the \emph{flattening of $\D$ with relation to $\mathcal{I}$} as the distribution $\Psi_{\mathcal{I}}(\D)$, where $\Psi_{\mathcal{I}}(\D)(i) = {\D(I_k)}/{\abs{I_k}}$ for all $k\in [\ell]$ and $i\in I_k$. A straightforward computation shows that such flattening cannot increase the distance between two distributions, i.e.,
  \begin{equation}\label{eq:Birge:tv}
      \totalvardist{ \Psi_{\mathcal{I}}(\D_1) }{ \Psi_{\mathcal{I}}(\D_2) } \leq \totalvardist{\D_1}{\D_2}.
  \end{equation}
\begin{proof}[Proof of Eq.~\eqref{eq:Birge:tv}]
    Fix a partition $\mathcal{I}$ of $[n]$ into $\ell$ intervals $I_1,\dots, I_\ell$, and let $\D_1$, $\D_2$ be two arbitrary distributions on $[n]$. Recall that $\Psi_{\mathcal{I}}(\D)$ is the flattening of distribution $\D_j$ (with relation to the partition $\mathcal{I}$).
      \begin{align*}
      2\totalvardist{ \Psi_{\mathcal{I}}(\D_1) }{ \Psi_{\mathcal{I}}(\D_2) } &= \sum_{i=1}^n \abs{ \Psi_{\mathcal{I}}(\D_1)(i) - \Psi_{\mathcal{I}}(\D_2)(i) } = \sum_{k=1}^\ell \sum_{i\in I_k} \abs{ \frac{\D_1(I_k)}{\abs{I_k}} - \frac{\D_2(I_k)}{\abs{I_k}} } \\
      &=  \sum_{k=1}^\ell \abs{ \D_1(I_k) - \D_2(I_k) } = \sum_{k=1}^\ell \abs{ \sum_{i\in I_k}\left( \D_1(i) - \D_2(i) \right) } \\
      &\leq  \sum_{k=1}^\ell \sum_{i\in I_k} \abs{ \D_1(i) - \D_2(i)  } = \sum_{i=1}^n \abs{ \D_1(i) - \D_2(i)  } 
      = 2\totalvardist{\D_1}{\D_2}.
      \end{align*}
(we remark that Eq.~\eqref{eq:Birge:tv} could also be obtained directly by applying the data processing inequality for total variation distance (\cref{lemma:data:processing:inequality:total:variation}) to $\D_1$, $\D_2$, for the transformation $\Psi_{\mathcal{I}}(\cdot)$.)
\end{proof}

We state here a few facts about monotone distributions, namely that they admit a \emph{succinct} approximation, itself monotone, close in total variation distance. This theorem, originally from \cite{Birge:87}, has recently been pivotal in several results on learning and testing $k$-modal distributions \cite{DDS:12,DDSVV:13}. 
\begin{definition}[Birg\'e decomposition]\label{def:birge:decomposition}
  Given a parameter $\alpha>0$, the corresponding (oblivious) \emph{Birg\'e decomposition of $[n]$} is the partition $\mathcal{I}_\alpha=(I_1,\dots,I_\ell)$, where $\ell=\bigTheta{\frac{\ln( \alpha n + 1)}{\alpha}}=\bigTheta{\frac{\log n }{\alpha}}$ and $\abs{I_{k}}=\flr{(1+\alpha)^k}$, $1\leq k \leq \ell$. 
\end{definition}
Note that this partition consists of logarithmically many intervals \emph{and crucially only depends on $n$ and $\eps$} (and not on any specific distribution $\D$): for this reason, we will often refer to it as the ``oblivious'' decomposition.

\noindent {For a distribution $\D$ and parameter $\alpha$, define $\birge[\D]{\alpha}$ to be the ``flattened'' distribution with relation to the oblivious decomposition $\mathcal{I}_\alpha$, that is $\birge[\D]{\alpha} = \Psi_{\mathcal{I}_\alpha}(\D)$.} 
The next theorem states that every monotone distribution can be well-approximated by its flattening on the Birg\'e decomposition's intervals:     
\begin{theorem}[\cite{Birge:87,DDSVV:13}]\label{theorem:Birge:obl:decomp}
 If $\D$ is monotone, then $\totalvardist{\D}{\birge[\D]{\alpha}} \leq \alpha$.
\end{theorem}

\noindent As a corollary, one can extend the theorem to distributions only promised to be \emph{close} to monotone:
\begin{restatable}{corollary}{birgerobustcorollary}\label{coro:Birge:decomposition:robust}
    Suppose $\D$ is \eps-close to monotone, and let $\alpha > 0$. Then $\totalvardist{\D}{ \birge[\D]{\alpha} } \leq 2\eps + \alpha$. Furthermore,  $\birge[\D]{\alpha}$ is also \eps-close to monotone.
\end{restatable}
\begin{proof}[Proof of~\cref{coro:Birge:decomposition:robust}]
  Let $\D$ be \eps-close to monotone, and $\D^\prime$ be a monotone distribution such that $\totalvardist{\D}{\D^\prime} = \eta \leq \eps$. By Eq.~\eqref{eq:Birge:tv}, we have
  \begin{equation}\label{eqn:phialpha:cannot:increase:tv}
      \totalvardist{ \birge[\D]{\alpha} }{ \birge[\D^\prime]{\alpha} } \leq \totalvardist{\D}{\D^\prime} = \eta
  \end{equation}
  proving the last part of the claim  (since $\birge[\D^\prime]{\alpha}$ is easily seen to be monotone).

  \noindent Now, by the triangle inequality,
  \begin{align*}
      \totalvardist{\D}{ \birge[\D^\prime]{\alpha} } &\leq \totalvardist{\D}{\D^\prime} + \totalvardist{\D^\prime}{ \birge[\D^\prime]{\alpha} } + \totalvardist{ \birge[\D^\prime]{\alpha} }{ \birge[\D]{\alpha} } \\
      &\leq \eta + \alpha + \eta \\
      &\leq 2\eps+\alpha
  \end{align*}
  where the last inequality uses the assumption on $\D^\prime$ and~\cref{theorem:Birge:obl:decomp} applied to it.
\end{proof}
  
We now restate a result of Batu et al. relating closeness to uniformity in $\lp[2]$ and $\lp[1]$ norms to ``overall flatness'' of the probability mass function, and which will be one of the ingredients of the proof of \cref{theo:main:testing}:
\begin{lemma}[{\cite{BFRSW:00,BFFKRW:01}}]\label{lemma:small:l2:close:uniform:l1}
Let $\D$ be a distribution on a domain $S$. \textsf{(a)} If $\max_{i\in S} \D(i) \leq (1+\eps)\min_{i\in S} \D(i)$, then $\normtwo{\D}^2 \leq (1+\eps^2)/\abs{S}$. \textsf{(b)} If $\normtwo{\D}^2 \leq (1+\eps^2)/\abs{S}$, then $\normone{\D-\uniform_{S}} \leq \eps$.
\end{lemma}

\noindent Some of our algorithms will need to check that condition \textsf{(b)} above holds. To do so, they rely on the following, which one can derive from the techniques in~\cite{DKN:15} and whose proof we defer to \cref{app:l2:proof}:\todonote{Deal with this.}
\begin{restatable}[Adapted from {\cite[Theorem 11]{DKN:15}}]{lemma}{lemmaestimateltwoadd}\label{lemma:estimate:l2:add}
There exists an algorithm \textsc{Check-Small-$\lp[2]$} which, given parameters $\eps,\delta\in(0,1)$ and $c\cdot{\sqrt{\abs{I}}}/{\eps^2} \log(1/\delta)$ independent samples from a distribution $\D$ over $I$ (for some absolute constant $c>0$), outputs either \yes or \no, and satisfies the following.
  \begin{itemize}
    \item If $\normtwo{\D-\uniform_I} > {\eps}/{\sqrt{\abs{I}}}$, then the algorithm outputs \no with probability at least $1-\delta$;
    \item If $\normtwo{\D-\uniform_I} \leq {\eps}/{2\sqrt{\abs{I}}}$, then the algorithm outputs \yes with probability at least $1-\delta$.
  \end{itemize}
\end{restatable}

Finally, recall the following well-known result on distinguishing biased coins (which can for instance be derived from Eq.~(2.15) and~(2.16) of~\cite{AdellJodra:06}), that shall come in handy in proving our lower bounds:
\begin{fact}\label{fact:fair:biased:coin}
Let $p\in[\eta, 1-\eta]$ for some fixed constant $\eta > 0$, and suppose $m\leq\frac{c_\eta}{\eps^2}$, with $c_\eta$ a sufficiently small constant
and $\eps < \eta.$ Then,
\[ \totalvardist{ \binomial{m}{p} }{ \binomial{m}{p+\eps} } < \frac{1}{3}. \]
\end{fact}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tools from Analysis and Probability}
We first give several variants of the Chernoff bounds (see e.g.~\cite[Chapter~4]{MotwaniRaghavan:95}), which we will use extensively in this thesis.
\begin{theorem} \label{thm:multCB}
Let $Y_1,\dots,Y_m$ be $m$ independent random variables that take on values in $[0,1]$, where $\expect{Y_i} = p_i$, and $\sum_{i=1}^m p_i = P$. For any $\gamma \in (0,1]$ we have
\begin{align}
  \label{eq:additive-chernoff}\text{(additive bound)} & & \probaOf{ \sum_{i=1}^m Y_i   > P+ \gamma m  },\ \probaOf{ \sum_{i=1}^m Y_i  < P - \gamma m } &\leq \exp(-2 \gamma^2 m)\\
  \label{eq:cher-ub}\text{(multiplicative bound)}     & & \probaOf{ \sum_{i=1}^m Y_i > (1+\gamma)P } &< \exp(-\gamma^2 P/3)\\
  \text{and}\notag\\
  \label{eq:cher-lb}\text{(multiplicative bound)}     & & \probaOf{ \sum_{i=1}^m Y_i < (1-\gamma)P } &< \exp(-\gamma^2 P/2).
\end{align}
The bound in~\cref{eq:cher-ub} is derived from the following more general bound, which holds from any $\gamma > 0$:
  \begin{equation}\label{eq:cher-ub-gen}
  \probaOf{ \sum_{i=1}^m Y_i > (1+\gamma)P } \leq \left(\frac{e^\gamma}{(1+\gamma)^{1+\gamma}}\right)^{P}\;,
  \end{equation}
and which also implies that for any $B > 2eP$,
  \begin{equation}\label{eq:cher-ub-large}
  \probaOf{ \sum_{i=1}^m Y_i > B } \leq 2^{-B}\;.
  \end{equation}
\end{theorem}
The following extension of the multiplicative bound is useful when we only have upper and/or lower bounds on $P$ (see e.g.~\cite[Exercise~1.1]{DP:09}):
\begin{claim} \label{cor:CB-upperlower}
In the setting of~\cref{thm:multCB} suppose that $P_L \leq P \leq P_H.$ Then for any $\gamma \in (0,1]$, we have
  \begin{align}
    \probaOf{ \sum_{i=1}^m Y_i > (1+\gamma)P_H } &< \exp(-\gamma^2 P_H/3)      \label{eq:multCB-upper2}\\
    \probaOf{ \sum_{i=1}^m Y_i < (1-\gamma)P_L } &< \exp(-\gamma^2 P_L/2)      \label{eq:multCB-lower}
  \end{align}
\end{claim}

\noindent We will also rely on the following corollary of~\cref{thm:multCB}:
\begin{corollary}\label{cor:sum-wiXi}
Let $0 \leq w_1,\dots,w_m \in \R$ be such that $w_i \leq \kappa$ for all $i \in [m]$, where $\kappa \in (0,1]$. Let $X_1,\dots,X_m$ be i.i.d. Bernoulli random variables with $\Pr[X_i=1]=1/2$ for all $i$, and let \mbox{$X = \sum_{i=1}^m w_i X_i$} and $W = \sum_{i=1}^m w_i$.
For any $\gamma \in (0,1]$,
\[
\probaOf{ X > (1+\gamma)\frac{W}{2} }
    < \exp\left(-\gamma^2\frac{W}{6\kappa}\right)
    \;\mbox{ and }\;
\probaOf{ X < (1-\gamma)\frac{W}{2} }
    < \exp\left(-\gamma^2\frac{W}{4\kappa}\right)\;,
\]
and for any $B > e\cdot W$,
\[
\Pr[X > B] < 2^{-B/\kappa}\;.
\]
\end{corollary}
\begin{proof}
Let $w_i' \eqdef w_i/\kappa$ (so that $w_i' \in [0,1]$), $W' \eqdef \sum_{i=1}^m w'_i = W/\kappa$, and
for each $i \in [m]$ let $Y_i \eqdef w_i' X_i$, so that $Y_i$ takes on values in $[0,1]$ and $\expect{Y_i} = w'_i/2$.
Let $X' = \sum_{i=1}^m w'_i X_i = \sum_{i=1}^m Y_i$,
so that $\expect{X'} = W'/2$.
By the definitions of $W'$ and $X'$ and by~\cref{eq:cher-ub},
for any $\gamma \in (0,1]$,
\[
\probaOf{ X > (1+\gamma)\frac{W}{2} }
 = \probaOf{ X' > (1+\gamma)\frac{W'}{2} }
    < \exp\left(-\gamma^2\frac{W'}{6}\right)
    = \exp\left(-\gamma^2\frac{W}{6\kappa}\right),
\]
and similarly by~\cref{eq:cher-lb}
\[
\probaOf{ X < (1-\gamma)\frac{W}{2} }
    < \exp\left(-\gamma^2\frac{W}{{4}\kappa}\right)\;.
\]
For $B > e\cdot W = 2e\cdot W/2$
we apply~\cref{eq:cher-ub-large} and get
\[
\probaOf{ X > B }
 = \probaOf{ X' > B/\kappa }
    < 2^{-B/\kappa},
\]
as claimed.
\end{proof}

Next, we state a standard probabilistic result that some of our proofs will rely on, the Paley--Zygmund anticoncentration inequality:
\begin{theorem}[Paley--Zygmund inequality]\label{theo:paley:zigmund}
  Let $X$ be a non-negative random variable with finite variance. Then, for any $\theta\in[0,1]$,
  \[
      \probaOf{ X > \theta\expect{X} } \geq (1-\theta)^2\frac{\expect{X}^2}{\expect{X^2}}.
  \]
\end{theorem}

We also recall a classical inequality for sums of independent random variables, due to Bennett~\cite[Chapter 2]{Boucheron:13}:
\begin{theorem}[Bennett's inequality]
Let $X=\sum_{i=1}^n X_i$, where $X_1,\dots,X_n$ are independent random variables such that (i) $\shortexpect[X_i]=0$ and (ii) $\abs{X_i}\leq \alpha$ almost surely for all $1\leq i\leq n$. Letting $\sigma^2=\var[X]$, we have, for every $t\geq 0$,
\[
    \Pr[ X > t ] \leq \exp\left( -\frac{\var[X]}{\alpha^2} \vartheta\!\left( \frac{\alpha t}{\var[X]} \right) \right)
\]
where $\vartheta(x)=(1+x)\ln(1+x) - x$.
\end{theorem}

We will also require the following version of the rearrangement inequality, due to Hardy and Littlewood (cf. for instance~\cite[Theorem 2.2]{BennettS:88}):
  \begin{theorem}[Hardy--Littlewood Inequality]\label{theo:hardy:littlewood}
    Fix any $f,g\colon\R\to [0,\infty)$ such that $\lim_{\pm\infty} f = \lim_{\pm\infty} g = 0$. Then,
    \[
        \int_{\R} fg \leq \int_{\R} f^\ast g^\ast
    \]
    where $f^\ast, g^\ast$ denote the symmetric decreasing rearrangements of $f,g$ respectively.
  \end{theorem}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discrete Fourier transform}
For our SIIRV testing algorithm, we will need the following definition of the Fourier transform. 

\begin{definition}[Discrete Fourier Transform]
For $x \in \R$, we let $e(x) \eqdef  \exp(-2i\pi x)$. The \emph{Discrete Fourier Transform (DFT) modulo $M$} of a function
$F\colon\modulo{n} \to \C$ is  the function $\fourier{F}\colon\modulo{M}\to \C$ defined as
\[
    \fourier{F}(\xi)=\sum_{j=0}^{n-1} e\!\left(\frac{\xi j}{M}\right) F(j)
\]
for $\xi \in \modulo{M}$. The DFT modulo $M$ of a distribution $\p$, $\fourier{\p}$, is then the DFT modulo $M$ of its probability mass function (note that one can then equivalently see $\fourier{\p}(\xi)$ as the expectation $\fourier{\p}(\xi) = \shortexpect_{X\sim F}[e\!\left(\frac{\xi X}{M}\right)]$, for $\xi\in\modulo{M}$).

The \emph{inverse DFT modulo $M$} onto the range $[m,m+M-1]$ of $\fourier{F}\colon \modulo{M} \to \C$, is the function $F\colon [m, m+M-1] \cap \Z \to \C$ defined by 
\[
    F(j)= \frac{1}{M} \sum_{\xi=0}^{M-1} e\!\left(-\frac{\xi j}{M}\right) \fourier{F}(\xi),
\]
for $j \in [m, m+M-1] \cap \Z$.
\end{definition}

Note that the DFT (modulo $M$) is a linear operator; moreover, we recall the standard fact relating the norms of a function and of its Fourier transform, that we will use extensively:
\begin{theorem}[Plancherel's Theorem]
For $M\geq 1$ and $F,G\colon\modulo{n} \to \C$, we have (i) $\sum_{j=0}^{n-1} F(j)\overline{G(j)} =  \frac{1}{M}\sum_{\xi=0}^{M-1} \fourier{F}(\xi)\overline{\fourier{G}(\xi)}$; and (ii) $\normtwo{F}= \frac{1}{\sqrt{M}}\normtwo{\fourier{F}}$, 
where $\fourier{F},\fourier{G}$ are the DFT modulo $M$ of $F,G$, respectively.
\end{theorem}
\noindent(The latter equality is sometimes referred to as Parseval's theorem.) We also note that, for our PMD testing, we shall need the appropriate generalization of the Fourier transform to the multivariate setting. We leave this generalization to the corresponding section,~\cref{sec:pmd:testing}.

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Some more useful structural results}\label{sec:learn}
To establish the completeness of several of our algorithms, we will rely upon this lemma from~\cite{DKS:15}:
\begin{lemma}[{\cite[Lemma 2.3]{DKS:15}}]\label{lemma:FourierSupportLem}
Let $\p \in \classksiirv[n]{k}$ with $\sqrt{\var_{X \sim \p}[X]} = s$, $1/2>\delta>0$, and $M \in \Z_+$ with $M>s$.
Let $\fourier{\p}$ be the discrete Fourier transform of $\p$ modulo $M$. Then, we have
  \begin{enumerate}
    \item[(i)]\label{lemma:FourierSupportLem:i} Let $\mathcal{L} = \mathcal{L}(\delta, M,s) \eqdef \left\{ \xi \in [M-1] \mid \exists a, b \in \Z, 0 \leq a \leq b < k \textrm{ such that }
    |\xi/M - a/b| <  \frac{\sqrt{\ln (1/\delta)}}{2s}  \right\} \;.$ Then, $|\fourier{\p}(\xi)| \leq \delta$ for all $\xi \in [M-1] \setminus \mathcal{L}.$
    That is, $|\fourier{\p}(\xi)| > \delta$ for
    at most $|\mathcal{L}| \leq M k^2 s^{-1} \sqrt{\log(1/\delta)}$ values of $\xi$ .
    \item[(ii)]\label{lemma:FourierSupportLem:ii} At most $4Mks^{-1}\sqrt{\log(1/\delta)}$ many integers $0 \leq \xi \leq M-1$ have  $|\fourier{\p}(\xi)| > \delta \;.$
  \end{enumerate}
\end{lemma}


We then provide a simple structural lemma, bounding the $\lp[2]$ norm of any $(n,k)$-SIIRV as a function of $k$ and its variance only:
\begin{lemma}[Any $(n,k)$-SIIRV modulo $M$ has small {$\lp[2]$} norm]\label{claim:ksiirv:l2:norm}
  If $\p \in \mathcal{S}_{n, k}$ has variance $s^2$, then the distribution $\p'$ defined as $\p' \eqdef \p \bmod M$ satisfies 
  $
      \normtwo{\p'} \leq \sqrt{\frac{8k}{s}}
  $.
\end{lemma}
\begin{proof}[Proof of~\cref{claim:ksiirv:l2:norm}]
By Plancherel, we have
$
  \normtwo{\p'}^2 = \frac{1}{M} \sum_{\xi=0}^{M-1} \dabs{\fourier{\p'}(\xi)}^2 = \frac{1}{M} \sum_{\xi=0}^{M-1} \dabs{\fourier{\p}(\xi)}^2
$, 
the second equality due to the definition of $\fourier{\p'}$. Indeed, for any $\xi\in\modulo{M}$, 
\begin{align*}
    \fourier{\p'}(\xi) &= \sum_{j=0}^{M-1} e^{-2i\pi \frac{j\xi}{M}} \p'(j) = \sum_{j=0}^{M-1} e^{-2i\pi \frac{j\xi}{M}} \sum_{\substack{j'\in\mathbb{N}\\j' = j \bmod M}} \p(j')
    = \sum_{j=0}^{M-1} \sum_{\substack{j'\in\mathbb{N}\\j' = j \bmod M}} e^{-2i\pi \frac{j'\xi}{M}} \p(j')
    \\
    &= \sum_{j\in\mathbb{N}} e^{-2i\pi \frac{j'\xi}{M}} \p(j')
    = \fourier{\p}(\xi)
\end{align*}
as $u\mapsto e^{-2i\pi u}$ is $1$-periodic. Since $\abs{\fourier{\p}(\xi)}\leq 1$ for every $\xi\in\modulo{M}$ (as $\fourier{\p}(\xi) = \shortexpect_{j\sim \p}[e^{-2i\pi \frac{j\xi}{M}}]$), we can upper bound the RHS as
\[
    \frac{1}{M} \sum_{\xi=0}^{M-1} \dabs{\fourier{\p}(\xi)}^2 \leq \frac{1}{M} \sum_{r\geq 0} \sum_{\xi : \frac{1}{2^{r+1}} < \abs{\fourier{\p}(\xi)} \leq \frac{1}{2^{r}}} \abs{\fourier{\p}(\xi)}^2
    \leq \frac{1}{M} \sum_{r\geq 0} \frac{1}{2^{2r}} \abs{\setOfSuchThat{ \xi\in\modulo{M} }{ \frac{1}{2^{r+1}} < \abs{\fourier{\p}(\xi)} }}\;.
\]
Invoking~\cref{lemma:FourierSupportLem}(ii) with parameter $\delta$ set to $\frac{1}{2^{r+1}}$, we get that $\abs{\setOfSuchThat{ \xi\in\modulo{M} }{ \frac{1}{2^{r+1}} < \abs{\fourier{\p}(\xi)} }} \leq 4Mk s^{-1} \sqrt{r+1}$, from which \[
    \normtwo{\p'}^2 \leq \frac{4k}{s} \sum_{r\geq 0} \frac{\sqrt{r+1}}{2^{2r}} \leq \frac{8k}{s}
\]
as desired.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Error-Correcting Codes.} 

For an alphabet $\Sigma$, we denote the projection of $x \in \Sigma^n$ to a subset of coordinates $I \subseteq [n]$ by $x|_I$. For $i \in [n]$, we write $x_i = x|_{\{i\}}$ to denote the projection to a singleton. We denote the \emph{relative Hamming distance}, over alphabet $\Sigma$, between two strings $x \in \Sigma^n$ and $y \in \Sigma^n$ by $\dist{x}{y} \eqdef \abs{ \setOfSuchThat{ x_i \neq y_i }{ i \in [n] } }/n$.  Analogously to the distribution case, we say that $x$ is \emph{$\eps$-close} to $y$ if $\dist{x}{y} \leq \eps$, and otherwise we say that $x$ is \emph{$\eps$-far} from $y$. Similarly, we denote the \emph{relative Hamming distance} of $x$ from a non-empty set $S \subseteq \Sigma^n$ by $\dist{x}{S} \eqdef \min_{y \in S} \dist{x}{y})$. If $\dist{x}{S} \leq \eps$, we say that $x$ is \emph{$\eps$-close} to $S$, and otherwise we say that $x$ is \emph{$\eps$-far} from $S$.

Let $k,n\in\N$, and let $\Sigma$ be a finite alphabet. A \emph{code} is a one-to-one function $C\colon\Sigma^k \to \Sigma^n$ that maps \emph{messages} to \emph{codewords}, where $k$ and $n$ are called the code's \emph{dimension} and \emph{block length}, respectively. The \emph{rate} of the code, measuring the redundancy of the encoding, is defined to be $\rho \eqdef k/n$. We will sometime identify the code $C$ with its image $C(\Sigma^k)$. In particular, we shall write $c \in C$ to indicate that there exists $x\in\bitset^k$ such that $c = C(x)$, and say that $c$ is a codeword of $C$.  The \emph{relative distance} of a code is the minimal relative distance between two codewords of $C$, and is denoted by $\delta \eqdef \min_{c \neq c' \in C}\{\dist{c}{c'}\}$. 

We say that $C$ is an \emph{asymptotically good code} if it has constant rate and constant relative distance. We shall make an extensive use of asymptotically good codes that are \emph{balanced}, that is, codes in which each codeword consists of the same number of $0$'s and $1$'s

\begin{proposition}[Good Balanced Codes]\label{lemma:good:balanced:hamming:codes}
  For any constant $\delta \in [0,1/3)$, there exists a good balanced code $C\colon \bitset^k \to \bitset^n$ with relative distance $\delta$ and constant rate. Namely, there exists a constant $\rho>0$ such that the following holds.
  \begin{enumerate}[(i)]
    \item Balance: $\abs{C(x)} = \frac{n}{2}$ for all $x\in \bitset^k$;
    \item Relative distance: $\dist{C(x)}{C(y)} > \delta$ for all distinct $x,y\in \bitset^k$;
    \item Constant rate: $\frac{k}{n} \geq \rho$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  Fix any code $C'$ with linear distance $\delta$ and constant rate (denoted $\rho'$). We transform $C'\colon \bitset^k \to \bitset^{n'}$ to a balanced code $C\colon \bitset^k \to \bitset^{2n'}$ by representing $0$ and $1$ as the balanced strings $01$ and $10$ (respectively). More accurately, we let $C(x) \eqdef C'(x)\odot\overline{C'(x)}\in\bitset^n$ for all $x\in\bitset^k$, where  $\odot$ denotes the concatenation and $\bar{z}$ is the bitwise negation of $z$. It is immediate to check that this transformation preserves the distance, and that $C$ is a balanced code with rate $\rho\eqdef 2\rho'$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subparagraph{On uniformity.} For the sake of notation and clarity, throughout this dissertation we define all algorithms and objects non-uniformly. Namely, we fix the relevant parameter (typically $n\in\N$), and restrict ourselves to inputs or domains of size $n$ (for instance, probability distributions over domain $[n]$). However, we still view it as a generic parameter and allow ourselves to write asymptotic expressions such as $\bigO{n}$. Moreover, although our results are stated in terms of non-uniform algorithms, they can be extended to the uniform setting in a straightforward manner.

\paragraph{On the domain and parameters.} Unless specified otherwise, $\domain$ will hereafter by default be the $n$-element set $[n]$. When stating the results, the accuracy parameter $\eps\in(0,1]$ is to be understood as taking small values, either a fixed (small) constant or a quantity tending to $0$ as $n\to\infty$; however, the actual parameter of interest will always be $n$, viewed as ``going to infinity.'' Hence any dependence on $n$, no matter how mild, shall be considered as more expensive than any function of $\eps$ only.

\chapter*{Introduction} % Not a numbered chapter
\addcontentsline{toc}{chapter}{Introduction} % Puts your introduction in your table of contents even though we have used the asterisk in the \chapter command above.

\epigraph{``The thing can be done,'' said the Butcher, ``I think.\\
The thing must be done, I am sure.\\
The thing shall be done! Bring me paper and ink,\\
The best there is time to procure.''}{Lewis Carroll, \textit{The Hunting of the Snark}}

\section*{Our contribution}

\section*{Organization of the dissertation}

In~\cref{chap:preliminaries}, we lay down the necessary notation and definitions that will be used throughout this thesis, and state some results from the literature that we shall need afterwards. We will also prove there several simple results that will be relied upon in the other chapters, and more generally set up the board and pieces.~\cref{chap:unified:ub} then will be concerned with general strategies to play the game; or, put differently, with unified frameworks to obtain algorithmic \emph{upper bounds} on distribution testing questions. In more detail,~\cref{sec:shaperestrictions} describes a unified approach for testing membership in classes of distributions, particularly relevant for classes of \emph{shape-restricted} distributions; while~\cref{sec:fourier} contains a different approach for this question, well-suited for those classes of distributions which enjoy ``nice'' Fourier spectra. The first is based on joint work with Ilias Diakonikolas, Themis Gouleakis, and Ronitt Rubinfeld~\cite{CDGR:16}, and the second on the paper~\cite{CDS:17} with Ilias Diakonikolas and Alistair Stewart.

In~\cref{chap:unified:lb}, we complement these algorithmic frameworks by describing new general approaches to obtaining information-theoretic \emph{lower bounds} in distribution testing.~\cref{sec:learningreductions}, based on~\cite{CDGR:16}, describes a reduction technique which allows to lift hardness of testing a sub-property $\property'\subseteq\property$ to that of testing $\property$ itself, modulo a mild learnability condition on the latter. As a corollary, we obtain new (as well as previously known) lower bounds for many distribution classes, in a clean and unified way.~\cref{sec:communication} (based on the paper~\cite{BCG:17} with Eric Blais and Tom Gur) then provides another framework  to easily establish distribution testing lower bounds, this time by carrying over lower bounds from \emph{communication complexity}. We show how this reduction from communication complexity, besides enabling us to easily derive lower bounds for a variety of distribution testing questions, can also shed light on existing results, leading to an unexpected connection between distribution testing and the seemingly unrelated field of interpolation theory.

In these two chapters, we were concerned with the ``standard'' setting of distribution testing, which only assumes access to independent samples; and developed general methods to tackle questions in this setting. In~\cref{chap:newmodels}, we take a different path: instead of finding new strategies to play the game, we change the \emph{rules} themselves -- granting the testing algorithms a more powerful type of access to the unknown distribution. Based on a work with Dana Ron and Rocco Servedio~\cite{CRS:15},~\cref{sec:conditional} introduces and studies the \emph{conditional sampling model}, in which the algorithm can get samples from the underlying probability distribution conditioned on subsets of events of its choosing. In~\cref{sec:extended}, we define and study two different settings, the \emph{dual access} and \emph{cumulative dual access} models, in which one can both draw independent samples from the distribution and query the value on any point of the domain of either its probability mass function or cumulative distribution function. (This is based on the paper~\cite{CR:14}, with Ronitt Rubinfeld.) Both sections thus consider testing algorithms that are at least as powerful as those from the standard sampling setting; the question is to quantify \emph{how much} more powerful these algorithms can be, and what limitations remain.

Finally, in~\cref{chap:correction} we venture out of property testing to explore a different -- albeit related -- paradigm: that of distribution \emph{correcting}. Changing now the \emph{goal} of the game, we introduce the notion of sampling corrector: granted access to independent samples from a probability distribution only \emph{close} to having some property $\property$ of interest, one must provide access to samples from a ``corrected'' distribution which, while still being close to the original distribution, does satisfy $\property$. We prove general results on this new algorithmic primitive, and study its relation to both distribution learning and testing; before focusing specifically on correction of a well-studied property of distributions, monotonicity. This last chapter contains material from~\cite{CGR:16}, joint work with Themis Gouleakis and Ronitt Rubinfeld.

